{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from yolo import YoloBody\n",
    "from nets.yolo_training import (YOLOLoss, get_lr_scheduler, set_optimizer_lr,\n",
    "                                weights_init)\n",
    "from utils.dataloader import YoloDataset, yolo_dataset_collate\n",
    "from utils.utils import get_anchors, get_classes, show_config\n",
    "\n",
    "RANDOM_STATE = 11\n",
    "PATH_MODEL_DATA = \"./model_data/\"\n",
    "PATH_LOGS = \"./logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False\n",
    "classes_path = 'model_data/voc_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "model_path = 'model_data/yolo4_weights.pth'\n",
    "input_shape = [416, 416]\n",
    "\n",
    "train_annotation_path = '2007_train.txt'\n",
    "val_annotation_path = '2007_val.txt'\n",
    "\n",
    "class_names, num_classes = get_classes(classes_path)\n",
    "anchors, num_anchors = get_anchors(anchors_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = 4\n",
    "\n",
    "epochs = 70\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------#\n",
    "#   Init_lr         模型的最大学习率\n",
    "#   Min_lr          模型的最小学习率，默认为最大学习率的0.01\n",
    "#------------------------------------------------------------------#\n",
    "Init_lr             = 1e-2\n",
    "Min_lr              = Init_lr * 0.01\n",
    "#------------------------------------------------------------------#\n",
    "#   optimizer_type  使用到的优化器种类，可选的有adam、sgd\n",
    "#                   当使用Adam优化器时建议设置  Init_lr=1e-3\n",
    "#                   当使用SGD优化器时建议设置   Init_lr=1e-2\n",
    "#   momentum        优化器内部使用到的momentum参数\n",
    "#   weight_decay    权值衰减，可防止过拟合\n",
    "#                   adam会导致weight_decay错误，使用adam时建议设置为0。\n",
    "#------------------------------------------------------------------#\n",
    "optimizer_type      = \"sgd\"\n",
    "momentum            = 0.937\n",
    "weight_decay        = 5e-4\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "#   label_smoothing     标签平滑。一般0.01以下。如0.01、0.005。\n",
    "#------------------------------------------------------------------#\n",
    "label_smoothing     = 0\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "#   focal_loss      是否使用Focal Loss平衡正负样本\n",
    "#   focal_alpha     Focal Loss的正负样本平衡参数\n",
    "#   focal_gamma     Focal Loss的难易分类样本平衡参数\n",
    "#------------------------------------------------------------------#\n",
    "focal_loss          = False\n",
    "focal_alpha         = 0.25\n",
    "focal_gamma         = 2\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "#   iou_type        使用什么iou损失，ciou或者siou\n",
    "#------------------------------------------------------------------#\n",
    "iou_type            = 'ciou'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal type\n",
      "Load weights model_data/yolo4_weights.pth.\n",
      "\n",
      "Successful Load Key: ['backbone.conv1.conv.weight', 'backbone.conv1.bn.weight', 'backbone.conv1.bn.bias', 'backbone.conv1.bn.running_mean', 'backbone.conv1.bn.running_var', 'backbone.conv1.bn.num_batches_tracked', 'backbone.stages.0.downsample_conv.conv.weight', 'backbone.stages.0.downsample_conv.bn.weight', 'backbone.stages.0.downsample_conv.bn.bias', 'backbone.stages.0.downsample_conv.bn.running_mean', 'backbone.stages.0.downsample_conv.bn.running_var', 'backbone.stages.0.downsample_conv.bn.num_batches_tracked', ' ……\n",
      "Successful Load Key Num: 642\n",
      "\n",
      "Fail To Load Key: ['yolo_head3.1.weight', 'yolo_head3.1.bias', 'yolo_head2.1.weight', 'yolo_head2.1.bias', 'yolo_head1.1.weight', 'yolo_head1.1.bias'] ……\n",
      "Fail To Load Key num: 6\n",
      "\n",
      "\u001b[1;33;44m温馨提示，head部分没有载入是正常现象，Backbone部分没有载入是错误的。\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YoloBody(anchors_mask, num_classes, pretrained=pretrained)\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "if model_path != '':\n",
    "    print('Load weights {}.'.format(model_path))\n",
    "    \n",
    "    model_dict      = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_path, map_location = device)\n",
    "    load_key, no_load_key, temp_dict = [], [], {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
    "            temp_dict[k] = v\n",
    "            load_key.append(k)\n",
    "        else:\n",
    "            no_load_key.append(k)\n",
    "    model_dict.update(temp_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"……\\nSuccessful Load Key Num:\", len(load_key))\n",
    "    print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"……\\nFail To Load Key num:\", len(no_load_key))\n",
    "    print(\"\\n\\033[1;33;44m温馨提示，head部分没有载入是正常现象，Backbone部分没有载入是错误的。\\033[0m\")\n",
    "\n",
    "yolo_loss = YOLOLoss(anchors, num_classes, input_shape, True, anchors_mask, label_smoothing, focal_loss, focal_alpha, focal_gamma, iou_type)\n",
    "seg_loss = nn.CrossEntropyLoss()\n",
    "obj_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "seg_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model_train = model.train().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC_2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_annotation_path, encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "with open(val_annotation_path, encoding='utf-8') as f:\n",
    "    val_lines   = f.readlines()\n",
    "num_train = len(train_lines)\n",
    "num_val = len(val_lines)\n",
    "\n",
    "\n",
    "train_yolo_dataset = YoloDataset(train_lines, input_shape, num_classes, epoch_length=epochs, train=True)\n",
    "val_yolo_dataset = YoloDataset(val_lines, input_shape, num_classes, epoch_length=epochs, train=False)\n",
    "\n",
    "train_yolo_loader = DataLoader(train_yolo_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers,\n",
    "                          pin_memory=True, drop_last=True, collate_fn=yolo_dataset_collate)\n",
    "val_yolo_loader = DataLoader(val_yolo_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers,\n",
    "                        pin_memory=True, drop_last=True, collate_fn=yolo_dataset_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6973"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_yolo_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADE20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ADE20K = np.load('../ADE20K_DL_course/ADE20K_DL_seg.npz')\n",
    "train_ade_X, train_ade_y = np_ADE20K['train_X'], np_ADE20K['train_y']\n",
    "val_ade_X, val_ade_y = np_ADE20K['val_X'], np_ADE20K['val_y']\n",
    "test_ade_X, test_ade_y = np_ADE20K['test_X'], np_ADE20K['test_y']\n",
    "del np_ADE20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ade_X, train_ade_y = torch.Tensor(train_ade_X), torch.Tensor(train_ade_y)\n",
    "val_ade_X, val_ade_y = torch.Tensor(val_ade_X), torch.Tensor(val_ade_y)\n",
    "test_ade_X, test_ade_y = torch.Tensor(test_ade_X), torch.Tensor(test_ade_y)\n",
    "\n",
    "train_ade_X = train_ade_X.permute([0, 3, 1, 2])\n",
    "val_ade_X = val_ade_X.permute([0, 3, 1, 2])\n",
    "test_ade_X = test_ade_X.permute([0, 3, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ade_dataset = TensorDataset(train_ade_X, train_ade_y)\n",
    "val_ade_dataset = TensorDataset(val_ade_X, val_ade_y)\n",
    "test_ade_dataset = TensorDataset(test_ade_X, test_ade_y)\n",
    "\n",
    "# adjust ade batch_size to match alternative learning\n",
    "# ade_batch_size = len(train_ade_dataset) // (len(train_yolo_dataset) // batch_size)\n",
    "ade_batch_size = 16\n",
    "train_ade_loader = DataLoader(train_ade_dataset, shuffle=True, batch_size=ade_batch_size, num_workers=num_workers, drop_last=True)\n",
    "val_ade_loader = DataLoader(val_ade_dataset, shuffle=True, batch_size=ade_batch_size, num_workers=num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1400, 3, 416, 416]), tensor(149.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ade_X.shape, train_ade_y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 87, 31, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_yolo_loader), len(train_ade_loader), len(val_yolo_loader), len(val_ade_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_epoch(model, obj_optimizer, seg_optimizer, obj_loss, seg_loss,\n",
    "                  train_yolo_loader, val_yolo_loader, train_ade_loader, val_ade_loader,\n",
    "                  e, epochs, batch_size):\n",
    "    # training\n",
    "    model.train()\n",
    "    loss_seg_total, loss_obj_total = 0, 0\n",
    "    pbar = tqdm(desc=f'Epoch {e+1} / {epochs}: Train', total=len(train_yolo_loader))\n",
    "    train_ade_loader_iter = iter(train_ade_loader)\n",
    "    for batch_i, (X_yolo, y_yolo) in enumerate(train_yolo_loader):\n",
    "        # object detection\n",
    "        X_yolo, y_yolo = X_yolo.to(device), [_.to(device) for _ in y_yolo]\n",
    "        obj_optimizer.zero_grad()\n",
    "        output = model(X_yolo)\n",
    "        yolo_ouput = output[:3]\n",
    "\n",
    "        loss_value = 0\n",
    "        for l in range(len(yolo_ouput)):\n",
    "            loss_item = obj_loss(l, yolo_ouput[l], y_yolo)\n",
    "            loss_value += loss_item\n",
    "        loss_value.backward()\n",
    "        obj_optimizer.step()\n",
    "        loss_obj_total += loss_value.item()\n",
    "\n",
    "        # semantic segmentation\n",
    "        try:\n",
    "            X_ade, y_ade = next(train_ade_loader_iter)\n",
    "        except StopIteration:\n",
    "            train_ade_loader_iter = iter(train_ade_loader)\n",
    "            X_ade, y_ade = next(train_ade_loader_iter)\n",
    "        X_ade, y_ade = X_ade.to(device), y_ade.to(device)\n",
    "        seg_optimizer.zero_grad()\n",
    "        output = model(X_ade)\n",
    "        segment_output = output[3]\n",
    "\n",
    "        loss_seg = seg_loss(segment_output, y_ade.to(torch.long))\n",
    "        loss_seg.backward()\n",
    "        seg_optimizer.step()\n",
    "        loss_seg_total += loss_seg.item()\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(train_obj_loss=loss_obj_total/(batch_i + 1), \n",
    "                         train_seg_loss=loss_seg_total/(batch_i + 1))\n",
    "    pbar.close()\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss_obj_total, val_loss_seg_total = 0, 0\n",
    "   \n",
    "    pbar = tqdm(desc=f'Epoch {e+1} / {epochs}: Val', total=len(val_yolo_loader))\n",
    "    val_ade_loader_iter = iter(val_ade_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (X_yolo, y_yolo) in enumerate(val_yolo_loader):\n",
    "            # object detection\n",
    "            X_yolo, y_yolo = X_yolo.to(device), [_.to(device) for _ in y_yolo]\n",
    "            obj_optimizer.zero_grad()\n",
    "            output = model(X_yolo)\n",
    "            yolo_ouput = output[:3]\n",
    "\n",
    "            loss_value = 0\n",
    "            for l in range(len(yolo_ouput)):\n",
    "                loss_item = obj_loss(l, yolo_ouput[l], y_yolo)\n",
    "                loss_value += loss_item\n",
    "            val_loss_obj_total += loss_value.item()\n",
    "\n",
    "            # semantic segmentation\n",
    "            try:\n",
    "                X_ade, y_ade = next(val_ade_loader_iter)\n",
    "            except StopIteration:\n",
    "                val_ade_loader_iter = iter(val_ade_loader)\n",
    "                X_ade, y_ade = next(val_ade_loader_iter)\n",
    "            X_ade, y_ade = X_ade.to(device), y_ade.to(device)\n",
    "            seg_optimizer.zero_grad()\n",
    "            output = model(X_ade)\n",
    "            segment_output = output[3]\n",
    "\n",
    "            loss_seg = seg_loss(segment_output, y_ade.to(torch.long))\n",
    "            val_loss_seg_total += loss_seg.item()\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(val_obj_loss=val_loss_obj_total/(batch_i + 1), \n",
    "                            val_seg_loss=val_loss_seg_total/(batch_i + 1))\n",
    "    pbar.close()\n",
    "    return {\n",
    "        \"epoch\": e+1,\n",
    "        \"train_obj_loss\": round(loss_obj_total / len(train_yolo_loader), 4),\n",
    "        \"train_seg_loss\": round(loss_seg_total / len(train_yolo_loader), 4),\n",
    "        \"val_obj_loss\": round(val_loss_obj_total / len(val_yolo_loader), 4),\n",
    "        \"val_seg_loss\": round(val_loss_seg_total / len(val_yolo_loader), 4),\n",
    "    }\n",
    "    \n",
    "\n",
    "# fit_one_epoch(model, obj_optimizer, seg_optimizer, yolo_loss, seg_loss,\n",
    "#               train_yolo_loader, val_yolo_loader, train_ade_loader, val_ade_loader,\n",
    "#               0, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_his(dict_all, start_datetime):\n",
    "    pd.DataFrame(dict_all).to_csv(Path(PATH_LOGS) / f'training_history_{start_datetime}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 70: Train: 100%|██████████| 217/217 [02:49<00:00,  1.28it/s, train_obj_loss=7.16, train_seg_loss=3.14]\n",
      "Epoch 1 / 70: Val: 100%|██████████| 31/31 [00:17<00:00,  1.81it/s, val_obj_loss=3.77, val_seg_loss=3.04]\n",
      "Epoch 2 / 70: Train: 100%|██████████| 217/217 [02:49<00:00,  1.28it/s, train_obj_loss=1.49, train_seg_loss=2.1] \n",
      "Epoch 2 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.68it/s, val_obj_loss=1.91, val_seg_loss=3.15]\n",
      "Epoch 3 / 70: Train: 100%|██████████| 217/217 [02:46<00:00,  1.31it/s, train_obj_loss=0.715, train_seg_loss=1.66]\n",
      "Epoch 3 / 70: Val: 100%|██████████| 31/31 [00:17<00:00,  1.82it/s, val_obj_loss=1.29, val_seg_loss=3]   \n",
      "Epoch 4 / 70: Train: 100%|██████████| 217/217 [02:49<00:00,  1.28it/s, train_obj_loss=0.45, train_seg_loss=1.36] \n",
      "Epoch 4 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.71it/s, val_obj_loss=1.06, val_seg_loss=3.05]\n",
      "Epoch 5 / 70: Train: 100%|██████████| 217/217 [02:47<00:00,  1.29it/s, train_obj_loss=0.324, train_seg_loss=1.18]\n",
      "Epoch 5 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.71it/s, val_obj_loss=0.925, val_seg_loss=2.99]\n",
      "Epoch 6 / 70: Train: 100%|██████████| 217/217 [02:49<00:00,  1.28it/s, train_obj_loss=0.252, train_seg_loss=1.05]\n",
      "Epoch 6 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.71it/s, val_obj_loss=0.835, val_seg_loss=3.07]\n",
      "Epoch 7 / 70: Train: 100%|██████████| 217/217 [02:51<00:00,  1.27it/s, train_obj_loss=0.208, train_seg_loss=0.927]\n",
      "Epoch 7 / 70: Val: 100%|██████████| 31/31 [00:17<00:00,  1.74it/s, val_obj_loss=0.766, val_seg_loss=3.18]\n",
      "Epoch 8 / 70: Train: 100%|██████████| 217/217 [02:54<00:00,  1.25it/s, train_obj_loss=0.175, train_seg_loss=0.825]\n",
      "Epoch 8 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.71it/s, val_obj_loss=0.729, val_seg_loss=3.31]\n",
      "Epoch 9 / 70: Train: 100%|██████████| 217/217 [02:52<00:00,  1.26it/s, train_obj_loss=0.152, train_seg_loss=0.743]\n",
      "Epoch 9 / 70: Val: 100%|██████████| 31/31 [00:17<00:00,  1.77it/s, val_obj_loss=0.695, val_seg_loss=3.24]\n",
      "Epoch 10 / 70: Train: 100%|██████████| 217/217 [02:48<00:00,  1.29it/s, train_obj_loss=0.135, train_seg_loss=0.677]\n",
      "Epoch 10 / 70: Val: 100%|██████████| 31/31 [00:17<00:00,  1.74it/s, val_obj_loss=0.65, val_seg_loss=3.32] \n",
      "Epoch 11 / 70: Train: 100%|██████████| 217/217 [02:48<00:00,  1.29it/s, train_obj_loss=0.121, train_seg_loss=0.626]\n",
      "Epoch 11 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.71it/s, val_obj_loss=0.627, val_seg_loss=3.17]\n",
      "Epoch 12 / 70: Train: 100%|██████████| 217/217 [02:49<00:00,  1.28it/s, train_obj_loss=0.111, train_seg_loss=0.588]\n",
      "Epoch 12 / 70: Val: 100%|██████████| 31/31 [00:18<00:00,  1.69it/s, val_obj_loss=0.611, val_seg_loss=3.73]\n",
      "Epoch 13 / 70: Train: 100%|██████████| 217/217 [03:10<00:00,  1.14it/s, train_obj_loss=0.102, train_seg_loss=0.539]\n",
      "Epoch 13 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.52it/s, val_obj_loss=0.598, val_seg_loss=3.4] \n",
      "Epoch 14 / 70: Train: 100%|██████████| 217/217 [03:15<00:00,  1.11it/s, train_obj_loss=0.0939, train_seg_loss=0.511]\n",
      "Epoch 14 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.582, val_seg_loss=3.27]\n",
      "Epoch 15 / 70: Train: 100%|██████████| 217/217 [03:23<00:00,  1.07it/s, train_obj_loss=0.09, train_seg_loss=0.482]  \n",
      "Epoch 15 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.47it/s, val_obj_loss=0.586, val_seg_loss=3.73]\n",
      "Epoch 16 / 70: Train: 100%|██████████| 217/217 [03:23<00:00,  1.06it/s, train_obj_loss=0.0838, train_seg_loss=0.46] \n",
      "Epoch 16 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.578, val_seg_loss=3.27]\n",
      "Epoch 17 / 70: Train: 100%|██████████| 217/217 [03:24<00:00,  1.06it/s, train_obj_loss=0.0793, train_seg_loss=0.44] \n",
      "Epoch 17 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.567, val_seg_loss=3.26]\n",
      "Epoch 18 / 70: Train: 100%|██████████| 217/217 [03:24<00:00,  1.06it/s, train_obj_loss=0.0768, train_seg_loss=0.426]\n",
      "Epoch 18 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.43it/s, val_obj_loss=0.551, val_seg_loss=3.5] \n",
      "Epoch 19 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0733, train_seg_loss=0.4]  \n",
      "Epoch 19 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.51it/s, val_obj_loss=0.542, val_seg_loss=3.86]\n",
      "Epoch 20 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0709, train_seg_loss=0.391]\n",
      "Epoch 20 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.522, val_seg_loss=3.47]\n",
      "Epoch 21 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0684, train_seg_loss=0.366]\n",
      "Epoch 21 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s, val_obj_loss=0.461, val_seg_loss=3.74]\n",
      "Epoch 22 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0661, train_seg_loss=0.362]\n",
      "Epoch 22 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.47it/s, val_obj_loss=0.463, val_seg_loss=3.66]\n",
      "Epoch 23 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.07it/s, train_obj_loss=0.0627, train_seg_loss=0.35] \n",
      "Epoch 23 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.49, val_seg_loss=3.56] \n",
      "Epoch 24 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0628, train_seg_loss=0.338]\n",
      "Epoch 24 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.45it/s, val_obj_loss=0.445, val_seg_loss=3.21]\n",
      "Epoch 25 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.07it/s, train_obj_loss=0.0617, train_seg_loss=0.334]\n",
      "Epoch 25 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.445, val_seg_loss=3.55]\n",
      "Epoch 26 / 70: Train: 100%|██████████| 217/217 [03:22<00:00,  1.07it/s, train_obj_loss=0.0594, train_seg_loss=0.323]\n",
      "Epoch 26 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s, val_obj_loss=0.486, val_seg_loss=3.55]\n",
      "Epoch 27 / 70: Train: 100%|██████████| 217/217 [03:23<00:00,  1.07it/s, train_obj_loss=0.058, train_seg_loss=0.314] \n",
      "Epoch 27 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.475, val_seg_loss=3.62]\n",
      "Epoch 28 / 70: Train: 100%|██████████| 217/217 [03:22<00:00,  1.07it/s, train_obj_loss=0.0562, train_seg_loss=0.298]\n",
      "Epoch 28 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s, val_obj_loss=0.491, val_seg_loss=3.79]\n",
      "Epoch 29 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.07it/s, train_obj_loss=0.0546, train_seg_loss=0.323]\n",
      "Epoch 29 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.52it/s, val_obj_loss=0.457, val_seg_loss=3.45]\n",
      "Epoch 30 / 70: Train: 100%|██████████| 217/217 [03:20<00:00,  1.08it/s, train_obj_loss=0.0547, train_seg_loss=0.288]\n",
      "Epoch 30 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.473, val_seg_loss=3.56]\n",
      "Epoch 31 / 70: Train: 100%|██████████| 217/217 [03:23<00:00,  1.07it/s, train_obj_loss=0.0549, train_seg_loss=0.289]\n",
      "Epoch 31 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.515, val_seg_loss=3.34]\n",
      "Epoch 32 / 70: Train: 100%|██████████| 217/217 [03:30<00:00,  1.03it/s, train_obj_loss=0.0551, train_seg_loss=0.29] \n",
      "Epoch 32 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.54, val_seg_loss=3.47] \n",
      "Epoch 33 / 70: Train: 100%|██████████| 217/217 [03:31<00:00,  1.03it/s, train_obj_loss=0.0515, train_seg_loss=0.277]\n",
      "Epoch 33 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.49it/s, val_obj_loss=0.458, val_seg_loss=3.61]\n",
      "Epoch 34 / 70: Train: 100%|██████████| 217/217 [03:24<00:00,  1.06it/s, train_obj_loss=0.0524, train_seg_loss=0.267]\n",
      "Epoch 34 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.47it/s, val_obj_loss=0.502, val_seg_loss=3.41]\n",
      "Epoch 35 / 70: Train: 100%|██████████| 217/217 [03:27<00:00,  1.05it/s, train_obj_loss=0.0508, train_seg_loss=0.257]\n",
      "Epoch 35 / 70: Val: 100%|██████████| 31/31 [00:22<00:00,  1.39it/s, val_obj_loss=0.522, val_seg_loss=3.72]\n",
      "Epoch 36 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0497, train_seg_loss=0.252]\n",
      "Epoch 36 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.48it/s, val_obj_loss=0.534, val_seg_loss=4.03]\n",
      "Epoch 37 / 70: Train: 100%|██████████| 217/217 [03:22<00:00,  1.07it/s, train_obj_loss=0.0498, train_seg_loss=0.25] \n",
      "Epoch 37 / 70: Val: 100%|██████████| 31/31 [00:30<00:00,  1.01it/s, val_obj_loss=0.527, val_seg_loss=3.51]\n",
      "Epoch 38 / 70: Train: 100%|██████████| 217/217 [04:37<00:00,  1.28s/it, train_obj_loss=0.0503, train_seg_loss=0.248]\n",
      "Epoch 38 / 70: Val: 100%|██████████| 31/31 [01:01<00:00,  2.00s/it, val_obj_loss=0.519, val_seg_loss=3.44]\n",
      "Epoch 39 / 70: Train: 100%|██████████| 217/217 [06:50<00:00,  1.89s/it, train_obj_loss=0.0476, train_seg_loss=0.248]\n",
      "Epoch 39 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s, val_obj_loss=0.444, val_seg_loss=3.31]\n",
      "Epoch 40 / 70: Train: 100%|██████████| 217/217 [03:20<00:00,  1.08it/s, train_obj_loss=0.0488, train_seg_loss=0.24] \n",
      "Epoch 40 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.51it/s, val_obj_loss=0.52, val_seg_loss=3.22] \n",
      "Epoch 41 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0479, train_seg_loss=0.24] \n",
      "Epoch 41 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.486, val_seg_loss=3.26]\n",
      "Epoch 42 / 70: Train: 100%|██████████| 217/217 [03:17<00:00,  1.10it/s, train_obj_loss=0.0472, train_seg_loss=0.242]\n",
      "Epoch 42 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.54it/s, val_obj_loss=0.511, val_seg_loss=3.13]\n",
      "Epoch 43 / 70: Train: 100%|██████████| 217/217 [03:13<00:00,  1.12it/s, train_obj_loss=0.0473, train_seg_loss=0.233]\n",
      "Epoch 43 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.47it/s, val_obj_loss=0.542, val_seg_loss=3.01]\n",
      "Epoch 44 / 70: Train: 100%|██████████| 217/217 [03:22<00:00,  1.07it/s, train_obj_loss=0.0478, train_seg_loss=0.226]\n",
      "Epoch 44 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.50it/s, val_obj_loss=0.521, val_seg_loss=3.24]\n",
      "Epoch 45 / 70: Train: 100%|██████████| 217/217 [03:21<00:00,  1.08it/s, train_obj_loss=0.0467, train_seg_loss=0.223]\n",
      "Epoch 45 / 70: Val: 100%|██████████| 31/31 [00:21<00:00,  1.46it/s, val_obj_loss=0.581, val_seg_loss=3.3] \n",
      "Epoch 46 / 70: Train: 100%|██████████| 217/217 [03:24<00:00,  1.06it/s, train_obj_loss=0.0457, train_seg_loss=0.223]\n",
      "Epoch 46 / 70: Val: 100%|██████████| 31/31 [00:20<00:00,  1.48it/s, val_obj_loss=0.535, val_seg_loss=3.08]\n",
      "Epoch 47 / 70: Train: 100%|██████████| 217/217 [05:11<00:00,  1.44s/it, train_obj_loss=0.0459, train_seg_loss=0.217]\n",
      "Epoch 47 / 70: Val: 100%|██████████| 31/31 [00:29<00:00,  1.04it/s, val_obj_loss=0.525, val_seg_loss=3.17]\n",
      "Epoch 48 / 70: Train: 100%|██████████| 217/217 [04:29<00:00,  1.24s/it, train_obj_loss=0.0464, train_seg_loss=0.221]\n",
      "Epoch 48 / 70: Val: 100%|██████████| 31/31 [00:29<00:00,  1.04it/s, val_obj_loss=0.541, val_seg_loss=3.09]\n",
      "Epoch 49 / 70: Train: 100%|██████████| 217/217 [04:29<00:00,  1.24s/it, train_obj_loss=0.0442, train_seg_loss=0.215]\n",
      "Epoch 49 / 70: Val: 100%|██████████| 31/31 [00:29<00:00,  1.04it/s, val_obj_loss=0.575, val_seg_loss=3.19]\n",
      "Epoch 50 / 70: Train: 100%|██████████| 217/217 [04:29<00:00,  1.24s/it, train_obj_loss=0.0446, train_seg_loss=0.207]\n",
      "Epoch 50 / 70: Val: 100%|██████████| 31/31 [00:30<00:00,  1.03it/s, val_obj_loss=0.593, val_seg_loss=3.28]\n",
      "Epoch 51 / 70: Train: 100%|██████████| 217/217 [04:16<00:00,  1.18s/it, train_obj_loss=0.0443, train_seg_loss=0.205]\n",
      "Epoch 51 / 70: Val: 100%|██████████| 31/31 [00:25<00:00,  1.20it/s, val_obj_loss=0.567, val_seg_loss=3.15]\n",
      "Epoch 52 / 70: Train: 100%|██████████| 217/217 [03:54<00:00,  1.08s/it, train_obj_loss=0.043, train_seg_loss=0.211] \n",
      "Epoch 52 / 70: Val: 100%|██████████| 31/31 [00:25<00:00,  1.20it/s, val_obj_loss=0.427, val_seg_loss=3.06]\n",
      "Epoch 53 / 70: Train: 100%|██████████| 217/217 [03:55<00:00,  1.09s/it, train_obj_loss=0.0431, train_seg_loss=0.235]\n",
      "Epoch 53 / 70: Val: 100%|██████████| 31/31 [00:27<00:00,  1.13it/s, val_obj_loss=0.495, val_seg_loss=3.29]"
     ]
    }
   ],
   "source": [
    "dict_all = {\n",
    "    \"epoch\": [], \n",
    "    \"train_obj_loss\": [], \"train_seg_loss\": [],\n",
    "    \"val_obj_loss\": [], \"val_seg_loss\": []\n",
    "}\n",
    "start_datetime = str(datetime.datetime.now()).split('.')[0].replace(' ', '_')\n",
    "for e in range(epochs):\n",
    "    best_obj_loss, best_seg_loss = 1e8, 1e8\n",
    "    dict_his = fit_one_epoch(\n",
    "        model, obj_optimizer, seg_optimizer, yolo_loss, seg_loss,\n",
    "        train_yolo_loader, val_yolo_loader, train_ade_loader, val_ade_loader,\n",
    "        e, epochs, batch_size\n",
    "    )\n",
    "    for k, v in dict_his.items():\n",
    "        dict_all[k].append(v)\n",
    "    save_his(dict_all, start_datetime)\n",
    "        \n",
    "    if dict_his['val_obj_loss'] < best_obj_loss:\n",
    "        best_obj_loss = dict_his['val_obj_loss']\n",
    "        torch.save(model.state_dict(), Path(PATH_MODEL_DATA) / f\"BestObjModel.pth\")\n",
    "        \n",
    "    if dict_his['val_seg_loss'] < best_seg_loss:\n",
    "        best_seg_loss = dict_his['val_seg_loss']\n",
    "        torch.save(model.state_dict(), Path(PATH_MODEL_DATA) / f\"BestSegModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': [30],\n",
       " 'train_obj_loss': [0.0544],\n",
       " 'train_seg_loss': [0.6453],\n",
       " 'val_obj_loss': [0.4132],\n",
       " 'val_seg_loss': [3.1825]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
